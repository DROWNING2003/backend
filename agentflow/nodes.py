import os
import re
from tools.search import TavilySearchTool
import yaml
from pocketflow import Node, BatchNode
from utils.call_llm import call_llm


def analyze_results(query, results):
    """Analyze search results using LLM"""
    if not results or not results.get('results'):
        return {
            "summary": "No search results to analyze",
            "key_points": [],
            "follow_up_queries": []
        }
    
    # Format results for LLM analysis
    formatted_results = ""
    for i, result in enumerate(results.get('results', [])[:5], 1):
        formatted_results += f"\n{i}. {result.get('title', 'No title')}\n"
        formatted_results += f"   URL: {result.get('url', 'No URL')}\n"
        formatted_results += f"   Content: {result.get('content', 'No content')[:500]}...\n"
    
    prompt = f"""
    åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œé’ˆå¯¹æŸ¥è¯¢: "{query}"
    
    æœç´¢ç»“æœ:
    {formatted_results}
    
    è¯·æä¾›:
    1. ç®€è¦æ€»ç»“ (2-3å¥è¯)
    2. å…³é”®è¦ç‚¹ (3-5ä¸ªè¦ç‚¹)
    3. å»ºè®®çš„åç»­æŸ¥è¯¢ (2-3ä¸ªç›¸å…³æŸ¥è¯¢)
    
    è¯·ä»¥JSONæ ¼å¼å›å¤:
    {{
        "summary": "æ€»ç»“å†…å®¹",
        "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
        "follow_up_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"]
    }}
    """
    
    try:
        response = call_llm(prompt)
        # Try to parse JSON response
        import json
        analysis = json.loads(response)
        return analysis
    except Exception as e:
        return {
            "summary": f"åˆ†æå¤±è´¥: {str(e)}",
            "key_points": ["æ— æ³•è§£ææœç´¢ç»“æœ"],
            "follow_up_queries": []
        }

class SearchNode(Node):
    """æœç´¢èŠ‚ç‚¹ç”¨äºè¡¥å……æŠ€æœ¯ç»†èŠ‚"""
    
    def prep(self, shared):
        return shared.get("query"), shared.get("num_results", 5)
        
    def exec(self, inputs):
        query, num_results = inputs
        if not query:
            return []
            
        searcher = TavilySearchTool()
        return searcher.search(query, num_results)
        
    def post(self, shared, prep_res, exec_res):
        shared["search_results"] = exec_res
        return "default"

class IdentifyAbstractions(Node):
    """è¯†åˆ«æŠ½è±¡æ¦‚å¿µèŠ‚ç‚¹ï¼šä½¿ç”¨LLMåˆ†æä»£ç ç»“æ„ï¼Œæå–æ ¸å¿ƒæ¦‚å¿µ,"""
    def prep(self, shared):
        # å‡†å¤‡é˜¶æ®µï¼šæ„å»ºLLMåˆ†ææ‰€éœ€çš„ä¸Šä¸‹æ–‡
        files_data = shared["files"]  # è·å–æ–‡ä»¶æ•°æ®
        project_name = shared["project_name"]  # ä»å…±äº«æ•°æ®è·å–é¡¹ç›®åç§°
        language = shared.get("language", "chinese")  # é»˜è®¤ä¸ºä¸­æ–‡è¾“å‡º
        use_cache = shared.get("use_cache", True)  # é»˜è®¤å¯ç”¨ç¼“å­˜
        max_abstraction_num = shared.get("max_abstraction_num", 5)  # é™åˆ¶æœ€å¤§æ¦‚å¿µæ•°é‡

        # æ ¼å¼åŒ–ä»£ç å†…å®¹ä¾›LLMåˆ†æ
        def create_llm_context(files_data):
            context = ""
            file_info = []  # å­˜å‚¨(ç´¢å¼•, è·¯å¾„)å…ƒç»„
            for i, (path, content) in enumerate(files_data.items()):
                # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ å¸¦ç´¢å¼•çš„æ ‡è®°
                entry = f"--- File Index {i}: {path} ---\n{content}\n\n"
                context += entry
                file_info.append((i, path))
            return context, file_info

        # ç”ŸæˆLLMæ‰€éœ€çš„ä¸Šä¸‹æ–‡å’Œæ–‡ä»¶åˆ—è¡¨
        context, file_info = create_llm_context(files_data)
        file_listing_for_prompt = "\n".join(
            [f"- {idx} # {path}" for idx, path in file_info]
        )
        
        # è¿”å›é¢„å¤„ç†ç»“æœå…ƒç»„
        return (
            context,
            file_listing_for_prompt,
            len(files_data),
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        )

    def exec(self, prep_res):
        # æ‰§è¡Œé˜¶æ®µï¼šè°ƒç”¨LLMè¿›è¡Œæ¦‚å¿µè¯†åˆ«
        (
            context,
            file_listing_for_prompt,
            file_count,
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        ) = prep_res  # è§£åŒ…é¢„å¤„ç†ç»“æœ

        # æ ¹æ®è¾“å‡ºè¯­è¨€é…ç½®æç¤ºè¯
        language_instruction = ""
        name_lang_hint = ""
        desc_lang_hint = ""
        if language.lower() != "english":
            # éè‹±è¯­è¾“å‡ºæ—¶éœ€è¦ç‰¹æ®Šæ ‡è®°
            language_instruction = f"IMPORTANT: Generate the `name` and `description` in **{language.capitalize()}**\n\n"
            name_lang_hint = f" ({language} output)"
            desc_lang_hint = f" ({language} output)"

        # æ„å»ºå®Œæ•´çš„LLMæç¤ºè¯
        prompt = f"""
è¯·åˆ†æé¡¹ç›® `{project_name}` çš„ä»£ç åº“ï¼š

ä»£ç ä¸Šä¸‹æ–‡ï¼š
{context}

{language_instruction}è¯·è¯†åˆ«5-{max_abstraction_num}ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š

æ¯ä¸ªæ¦‚å¿µéœ€è¦æä¾›ï¼š
1. `name`{name_lang_hint}ï¼šçŸ¥è¯†ç‚¹åç§°
2. `description`ï¼ˆçº¦100å­—ï¼‰{desc_lang_hint}ï¼šçŸ¥è¯†ç‚¹æè¿°
3. `file_indices`ï¼šç›¸å…³æ–‡ä»¶ç´¢å¼•åˆ—è¡¨

æ–‡ä»¶ç´¢å¼•å¯¹ç…§è¡¨ï¼š
{file_listing_for_prompt}

è¯·ç”¨YAMLæ ¼å¼è¾“å‡ºï¼š

```yaml
- name: |
    ç¤ºä¾‹æ¦‚å¿µ{name_lang_hint}
  description: |
    è¿™é‡Œæ˜¯è¯¥çŸ¥è¯†æ¦‚å¿µçš„è¯¦ç»†è¯´æ˜ï¼Œæè¿°å…¶æ ¸å¿ƒåŠŸèƒ½å’Œè®¾è®¡æ„å›¾ã€‚
    å»ºè®®ä½¿ç”¨ç±»æ¯”æ–¹å¼è§£é‡ŠæŠ€æœ¯åŸç†ã€‚{desc_lang_hint}
  file_indices:
    - 0 # æ–‡ä»¶è·¯å¾„ç¤ºä¾‹.py"""

        # è°ƒç”¨LLMå¹¶å¤„ç†å“åº”
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0))
        
        # æå–å’ŒéªŒè¯YAMLå“åº”
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        abstractions = yaml.safe_load(yaml_str)

        # éªŒè¯å“åº”æ•°æ®ç»“æ„
        if not isinstance(abstractions, list):
            raise ValueError("LLMåº”è¿”å›åˆ—è¡¨æ ¼å¼")

        validated_abstractions = []
        for item in abstractions:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not all(k in item for k in ["name", "description", "file_indices"]):
                raise ValueError("æŠ½è±¡æ¦‚å¿µç¼ºå°‘å¿…éœ€å­—æ®µ")
                
            # éªŒè¯æ–‡ä»¶ç´¢å¼•æœ‰æ•ˆæ€§
            valid_indices = []
            for idx_entry in item["file_indices"]:
                try:
                    idx = int(str(idx_entry).split("#")[0].strip())
                    if not 0 <= idx < file_count:
                        raise ValueError(f"æ–‡ä»¶ç´¢å¼•{idx}è¶Šç•Œ")
                    valid_indices.append(idx)
                except (ValueError, TypeError):
                    raise ValueError("æ— æ•ˆçš„æ–‡ä»¶ç´¢å¼•æ ¼å¼")

            # å­˜å‚¨éªŒè¯åçš„æ•°æ®
            validated_abstractions.append({
                "name": item["name"],
                "description": item["description"],
                "files": sorted(set(valid_indices))  # å»é‡æ’åº
            })

        return validated_abstractions

    def post(self, shared, prep_res, exec_res):
        print(exec_res)
        # å°†ç»“æœå­˜å…¥å…±äº«ä¸Šä¸‹æ–‡
        shared["knowledge"] = exec_res
        
class ToLevelConverter(Node):
    def prep(self, shared):
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True
        files_data = shared["files"]  # è·å–æ–‡ä»¶æ•°æ®
        project_name = shared["project_name"]  # ä»å…±äº«æ•°æ®è·å–é¡¹ç›®åç§°
        knowledge = shared["knowledge"]  # ä»å…±äº«æ•°æ®è·å–é¡¹ç›®åç§°
        language = shared.get("language", "chinese")
        
        language_instruction = ""
        name_lang_hint = ""
        desc_lang_hint = ""
        if language.lower() != "english":
            # éè‹±è¯­è¾“å‡ºæ—¶éœ€è¦ç‰¹æ®Šæ ‡è®°
            language_instruction = f"IMPORTANT: Generate the `name` and `description` in **{language.capitalize()}**\n\n"
            name_lang_hint = f" ({language} output)"
            desc_lang_hint = f" ({language} output)"
        return (
            language_instruction,
            desc_lang_hint,
            name_lang_hint,
            files_data,
            knowledge,
            use_cache,
            project_name,
            language,
        )
        
    def exec(self, prep_res):
        (
            use_cache,
            language_instruction,
            desc_lang_hint,
            name_lang_hint,
            files_data,
            knowledge,
            project_name,
            language,
          
        ) = prep_res  # Unpack use_cache
        prompt = f"""
è¯·æ ¹æ®é¡¹ç›® `{project_name}` çš„ä»£ç åº“è®¾è®¡ç¼–ç¨‹å­¦ä¹ å…³å¡ï¼š

ä»£ç ä¸Šä¸‹æ–‡ï¼š
{files_data}
çŸ¥è¯†ç‚¹ï¼š
{knowledge}

### å…³å¡è®¾è®¡è§„èŒƒ
æ¯ä¸ªçŸ¥è¯†ç‚¹å…³å¡éœ€åŒ…å«ï¼š
1. **çŸ¥è¯†ç‚¹å¼•å…¥** - ç”¨ç”Ÿæ´»æ¡ˆä¾‹ç±»æ¯”æŠ€æœ¯æ¦‚å¿µ
2. **ä»»åŠ¡è¦æ±‚** - å…·ä½“çš„ä»£ç å®ç°ç›®æ ‡
3. **ç¤ºä¾‹å‚è€ƒ** - å¯æ¨¡ä»¿çš„ä»£ç ç‰‡æ®µ

### è¾“å‡ºæ ¼å¼è¦æ±‚
```yaml
- name: |
    å…³å¡ä¸»é¢˜{name_lang_hint}
  description: |
    â–¸ çŸ¥è¯†ç‚¹ä»‹ç»
    â–¸ ç®€å•ä¾‹å­
    â–¸ è¯­æ³•è¯´æ˜
    â–¸ ç®€æ´æ˜äº†
  requirements: |
    â–¸ æŠŠä»£ç é€šè¿‡è¯­è¨€æè¿°
    â–¸ æè¿°ä¸åº”å¤ªè¿‡ç›´ç™½ï¼Œéœ€è¦æœ‰ç‚¹æŒ‘æˆ˜
    â–¸ ç”¨æˆ·å¯ä»¥æ ¹æ®æè¿°å®Œæˆå¤åˆ»ä»£ç 
```
### ç¤ºä¾‹
```yaml
- name: |
    æ•°ç»„
  description: |
    å¦‚æœä½ æƒ³å»ºç«‹ä¸€ä¸ªé›†åˆï¼Œå¯ä»¥ç”¨ _æ•°ç»„_ è¿™æ ·çš„æ•°æ®ç±»å‹ã€‚Solidity æ”¯æŒä¸¤ç§æ•°ç»„: _é™æ€_ æ•°ç»„å’Œ _åŠ¨æ€_ æ•°ç»„:
    ```solidity
    // å›ºå®šé•¿åº¦ä¸º2çš„é™æ€æ•°ç»„:
    uint[2] fixedArray;
    // å›ºå®šé•¿åº¦ä¸º5çš„stringç±»å‹çš„é™æ€æ•°ç»„:
    string[5] stringArray;
    // åŠ¨æ€æ•°ç»„ï¼Œé•¿åº¦ä¸å›ºå®šï¼Œå¯ä»¥åŠ¨æ€æ·»åŠ å…ƒç´ :
    uint[] dynamicArray;
    ```
    ä½ ä¹Ÿå¯ä»¥å»ºç«‹ä¸€ä¸ª ç»“æ„ä½“ç±»å‹çš„æ•°ç»„ï¼Œä¾‹å¦‚ï¼Œä¸Šä¸€ç« æåˆ°çš„ Person:
    ```solidity
    Person[] people; // è¿™æ˜¯åŠ¨æ€æ•°ç»„ï¼Œæˆ‘ä»¬å¯ä»¥ä¸æ–­æ·»åŠ å…ƒç´ 
    ```
    
    ## å…¬å…±æ•°ç»„
    ä½ å¯ä»¥å®šä¹‰ public æ•°ç»„ï¼ŒSolidity ä¼šè‡ªåŠ¨åˆ›å»º getter æ–¹æ³•ã€‚è¯­æ³•å¦‚ä¸‹:
    ```solidity
    Person[] public people;
    ```
    å…¶å®ƒçš„åˆçº¦å¯ä»¥ä»è¿™ä¸ªæ•°ç»„è¯»å–æ•°æ®ï¼ˆä½†ä¸èƒ½å†™å…¥æ•°æ®ï¼‰ï¼Œæ‰€ä»¥è¿™åœ¨åˆçº¦ä¸­æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ä¿å­˜å…¬å…±æ•°æ®çš„æ¨¡å¼ã€‚
  requirements: |
    ä¸ºäº†æŠŠä¸€ä¸ªåƒµå°¸éƒ¨é˜Ÿä¿å­˜åœ¨æˆ‘ä»¬çš„APPé‡Œï¼Œå¹¶ä¸”èƒ½å¤Ÿè®©å…¶å®ƒAPPçœ‹åˆ°è¿™äº›åƒµå°¸ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå…¬å…±æ•°ç»„ã€‚
    åˆ›å»ºä¸€ä¸ªæ•°æ®ç±»å‹ä¸º Zombie çš„ç»“æ„ä½“æ•°ç»„ï¼Œç”¨ public ä¿®é¥°ï¼Œå‘½åä¸ºï¼šzombiesã€‚
```
"""
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0))
        # --- Validation ---
        yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        Level_raw = yaml.safe_load(yaml_str)
        if not isinstance(Level_raw, list):
            raise ValueError("LLM output is not a list")
        print("__________________è¾“å‡º_______________________")
        print(Level_raw)
        return Level_raw
    
    def post(self, shared, prep_res, exec_res):
        """Save the search results and go back to the decision node."""
        # Add the search results to the context in the shared store
        # previous = shared.get("context", "")
        # shared["context"] = previous + "\n\nSEARCH: " + shared["search_query"] + "\nRESULTS: " + exec_res
        
        # print(f"ğŸ“š Found information, analyzing results...")
        
        # Always go back to the decision node after searching
        return