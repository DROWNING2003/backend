import logging
import tempfile
from agentflow.utils.yamltool import robust_yaml_parse
from agentflow.utils.crawl_github_files import clone_repository, get_or_clone_repository, filter_and_read_files, get_commit_changes_detailed, get_exclude_patterns, get_file_patterns, checkout_to_commit
from agentflow.tools.search import TavilySearchTool
import yaml
from pocketflow import Node, BatchNode
from agentflow.utils.call_llm import call_llm,call_MiniMax_llm
from agentflow.utils.token_manager import token_manager, safe_call_llm
logger = logging.getLogger(__name__)

def analyze_results(query, results):
    """Analyze search results using LLM"""
    if not results or not results.get('results'):
        return {
            "summary": "No search results to analyze",
            "key_points": [],
            "follow_up_queries": []
        }
    
    # Format results for LLM analysis
    formatted_results = ""
    for i, result in enumerate(results.get('results', [])[:5], 1):
        formatted_results += f"\n{i}. {result.get('title', 'No title')}\n"
        formatted_results += f"   URL: {result.get('url', 'No URL')}\n"
        formatted_results += f"   Content: {result.get('content', 'No content')[:500]}...\n"
    
    prompt = f"""
    åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œé’ˆå¯¹æŸ¥è¯¢: "{query}"
    
    æœç´¢ç»“æœ:
    {formatted_results}
    
    è¯·æä¾›:
    1. ç®€è¦æ€»ç»“ (2-3å¥è¯)
    2. å…³é”®è¦ç‚¹ (3-5ä¸ªè¦ç‚¹)
    3. å»ºè®®çš„åç»­æŸ¥è¯¢ (2-3ä¸ªç›¸å…³æŸ¥è¯¢)
    
    è¯·ä»¥JSONæ ¼å¼å›å¤:
    {{
        "summary": "æ€»ç»“å†…å®¹",
        "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
        "follow_up_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"]
    }}
    """
    
    try:
        response = call_llm(prompt)
        # Try to parse JSON response
        import json
        analysis = json.loads(response)
        return analysis
    except Exception as e:
        return {
            "summary": f"åˆ†æå¤±è´¥: {str(e)}",
            "key_points": ["æ— æ³•è§£ææœç´¢ç»“æœ"],
            "follow_up_queries": []
        }

class SearchNode(Node):
    """æœç´¢èŠ‚ç‚¹ç”¨äºè¡¥å……æŠ€æœ¯ç»†èŠ‚"""
    
    def prep(self, shared):
        return shared.get("query"), shared.get("num_results", 5)
        
    def exec(self, inputs):
        query, num_results = inputs
        if not query:
            return []
            
        searcher = TavilySearchTool()
        return searcher.search(query, num_results)
        
    def post(self, shared, prep_res, exec_res):
        shared["search_results"] = exec_res
        return "default"


class IdentifyAbstractions(Node):
    """æŠ½è±¡çŸ¥è¯†ç‚¹èŠ‚ç‚¹ï¼šä½¿ç”¨LLMåˆ†æä»£ç ç»“æ„ï¼Œæå–æ ¸å¿ƒçŸ¥è¯†ç‚¹,"""
    def prep(self, shared):
        print("ç°åœ¨æ˜¯åˆ†æèŠ‚ç‚¹")
        tmpdirname = shared["tmpdirname"]
        # repo = shared["repo"]
        # currentIndex = shared["currentIndex"]
        project_name = shared["project_name"]
        # checkout_to_commit(repo,currentIndex)
        
        #å…¨å±€åˆ†æä¼šå‡ºç°å½“å‰çŸ¥è¯†ç‚¹ä¸ä¹‹å‰çš„é‡åˆ
        result = filter_and_read_files(
            tmpdirname,
            max_file_size=1 * 1024 * 1024,
            include_patterns=get_file_patterns("code"),  # é¢„å®šä¹‰Pythonæ¨¡å¼
            exclude_patterns=get_exclude_patterns("common")  # æ’é™¤å¸¸è§æ— ç”¨æ–‡ä»¶
        )
        files = result["files"]
        shared["files"] = files
        #å¢é‡åˆ†æ
        # repo = shared["repo"]
        # currentIndex = shared["currentIndex"] 
        # get_commit_changes_detailed(repo,currentIndex)
        # å‡†å¤‡é˜¶æ®µï¼šæ„å»ºLLMåˆ†ææ‰€éœ€çš„ä¸Šä¸‹æ–‡

        language = shared.get("language", "chinese")  # é»˜è®¤ä¸ºä¸­æ–‡è¾“å‡º
        use_cache = shared.get("use_cache", True)  # é»˜è®¤å¯ç”¨ç¼“å­˜
        max_abstraction_num = shared.get("max_abstraction_num", 3)  # é™åˆ¶æœ€å¤§æ¦‚å¿µæ•°é‡
        print("æ–‡ä»¶å†…å®¹",files)
        
        # é¦–å…ˆæˆªæ–­æ–‡ä»¶å†…å®¹ä»¥æ§åˆ¶tokenæ•°é‡
        truncated_files = token_manager.truncate_files_content(files, max_tokens_per_file=3000)
        
        # æ ¼å¼åŒ–ä»£ç å†…å®¹ä¾›LLMåˆ†æ
        def create_llm_context(files_data):
            context = ""
            file_info = []  # å­˜å‚¨(ç´¢å¼•, è·¯å¾„)å…ƒç»„
            for i, (path, content) in enumerate(files_data.items()):
                # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ å¸¦ç´¢å¼•çš„æ ‡è®°
                entry = f"--- File Index {i}: {path} ---\n{content}\n\n"
                context += entry
                file_info.append((i, path))
            return context, file_info

        # ç”ŸæˆLLMæ‰€éœ€çš„ä¸Šä¸‹æ–‡å’Œæ–‡ä»¶åˆ—è¡¨
        context, file_info = create_llm_context(truncated_files)
        file_listing_for_prompt = "\n".join(
            [f"- {idx} # {path}" for idx, path in file_info]
        )
        
        # è¿”å›é¢„å¤„ç†ç»“æœå…ƒç»„
        return (
            context,
            file_listing_for_prompt,
            len(files),
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        )

    def exec(self, prep_res):
        # æ‰§è¡Œé˜¶æ®µï¼šè°ƒç”¨LLMè¿›è¡Œæ¦‚å¿µè¯†åˆ«
        (
            context,
            file_listing_for_prompt,
            file_count,
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        ) = prep_res  # è§£åŒ…é¢„å¤„ç†ç»“æœ

        # æ ¹æ®è¾“å‡ºè¯­è¨€é…ç½®æç¤ºè¯
        language_instruction = ""
        name_lang_hint = ""
        desc_lang_hint = ""
        if language.lower() != "english":
            # éè‹±è¯­è¾“å‡ºæ—¶éœ€è¦ç‰¹æ®Šæ ‡è®°
            language_instruction = f"IMPORTANT: Generate the `name` and `description` in **{language.capitalize()}**\n\n"
            name_lang_hint = f" ({language} output)"
            desc_lang_hint = f" ({language} output)"

        # æ„å»ºå®Œæ•´çš„LLMæç¤ºè¯
        prompt = f"""
è¯·åˆ†æé¡¹ç›® `{project_name}` çš„ä»£ç åº“ï¼š

ä»£ç ä¸Šä¸‹æ–‡ï¼š
{context}

{language_instruction}è¯·è¯†åˆ«5-{max_abstraction_num}ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š

æ¯ä¸ªæ¦‚å¿µéœ€è¦æä¾›ï¼š
1. `name`{name_lang_hint}ï¼šçŸ¥è¯†ç‚¹åç§°
2. `description`{desc_lang_hint}ï¼šçŸ¥è¯†ç‚¹æè¿°
3. `file_indices`ï¼šç›¸å…³æ–‡ä»¶ç´¢å¼•åˆ—è¡¨

æ–‡ä»¶ç´¢å¼•å¯¹ç…§è¡¨ï¼š
{file_listing_for_prompt}

è¯·ç”¨YAMLæ ¼å¼è¾“å‡ºï¼š

```yaml
- name: |
    ç¤ºä¾‹æ¦‚å¿µ{name_lang_hint}
  description: |
    è¿™é‡Œæ˜¯è¯¥çŸ¥è¯†æ¦‚å¿µçš„è¯¦ç»†è¯´æ˜ï¼Œæè¿°å…¶æ ¸å¿ƒåŠŸèƒ½å’Œè®¾è®¡æ„å›¾ã€‚
    å»ºè®®ä½¿ç”¨ç±»æ¯”æ–¹å¼è§£é‡ŠæŠ€æœ¯åŸç†ã€‚{desc_lang_hint}
  file_indices:
    - 0 # æ–‡ä»¶è·¯å¾„ç¤ºä¾‹.py"""

        # ä¼˜åŒ–promptä»¥æ§åˆ¶tokenæ•°é‡
        optimized_context, optimized_prompt = token_manager.optimize_prompt_for_abstractions(
            context, file_listing_for_prompt, prompt, max_context_tokens=100000
        )
        
        # é‡æ–°æ„å»ºæœ€ç»ˆprompt
        final_prompt = prompt.replace(context, optimized_context)
        
        # è°ƒç”¨LLMå¹¶å¤„ç†å“åº”
        response = safe_call_llm(final_prompt, use_cache)
        print(response)
        # æå–å’ŒéªŒè¯YAMLå“åº”
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        abstractions = yaml.safe_load(yaml_str)

        # éªŒè¯å“åº”æ•°æ®ç»“æ„
        if not isinstance(abstractions, list):
            raise ValueError("LLMåº”è¿”å›åˆ—è¡¨æ ¼å¼")

        validated_abstractions = []
        for item in abstractions:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not all(k in item for k in ["name", "description", "file_indices"]):
                raise ValueError("æŠ½è±¡æ¦‚å¿µç¼ºå°‘å¿…éœ€å­—æ®µ")
                
            # éªŒè¯æ–‡ä»¶ç´¢å¼•æœ‰æ•ˆæ€§
            valid_indices = []
            for idx_entry in item["file_indices"]:
                try:
                    idx = int(str(idx_entry).split("#")[0].strip())
                    if not 0 <= idx < file_count:
                        raise ValueError(f"æ–‡ä»¶ç´¢å¼•{idx}è¶Šç•Œ")
                    valid_indices.append(idx)
                except (ValueError, TypeError):
                    raise ValueError("æ— æ•ˆçš„æ–‡ä»¶ç´¢å¼•æ ¼å¼")

            # å­˜å‚¨éªŒè¯åçš„æ•°æ®
            validated_abstractions.append({
                "name": item["name"],
                "description": item["description"],
                "files": sorted(set(valid_indices))  # å»é‡æ’åº
            })

        return validated_abstractions

    def post(self, shared, prep_res, exec_res):
        print(exec_res)
        shared["knowledge"] = exec_res
        
class EvaluateContextWorthiness(Node):
    """è¯„ä¼°å½“å‰ä¸Šä¸‹æ–‡æ˜¯å¦å€¼å¾—ä½œä¸ºä¸€ä¸ªå…³å¡çš„èŠ‚ç‚¹"""
    
    def __init__(self):
        super().__init__()
        # self.max_commits_to_check = 4  # æœ€å¤šæ£€æŸ¥5ä¸ªè¿ç»­æäº¤
        self.min_code_changes = 5  # æœ€å°‘ä»£ç å˜æ›´è¡Œæ•°
        self.min_meaningful_files = 1  # æœ€å°‘æœ‰æ„ä¹‰çš„æ–‡ä»¶æ•°é‡
    
    def prep(self, shared):
        print("ç°åœ¨æ˜¯è¯„ä¼°èŠ‚ç‚¹")
        repo = shared["repo"]
        commits_to_check = shared["commits_to_check"]
        currentIndex = shared["currentIndex"]
        fullcommits = shared["fullcommits"]
        project_name = shared["project_name"]
        accumulated_changes = shared["accumulated_changes"]
        knowledge = shared["knowledge"]
        max_commits_to_check = shared["max_commits_to_check"]
        language = shared.get("language", "chinese")
        use_cache = shared.get("use_cache", True)
        
        return (commits_to_check,fullcommits,accumulated_changes,repo, currentIndex,max_commits_to_check,knowledge, project_name, language, use_cache)
    
    def exec(self, prep_res):
        (commits_to_check,fullcommits,accumulated_changes,repo,currentIndex,max_commits_to_check, knowledge,project_name, language, use_cache) = prep_res
        
        current_commit_index = currentIndex
        
        print("æ›´æ”¹",accumulated_changes)
        commits = list(fullcommits)
        print("commitsçš„é•¿åº¦",len(commits))
        print(f'è¿ç»­è¯»å–æ¬¡æ•°{commits_to_check},æœ€å¤§è¯»å–æ¬¡æ•°{max_commits_to_check}')
        if commits_to_check < max_commits_to_check:
            # if current_commit_index >= len(commits):
            #     break
                
            # è·å–å½“å‰æäº¤çš„è¯¦ç»†å˜æ›´
            detailed_changes = get_commit_changes_detailed(repo, current_commit_index, include_diff_content=True)
            
            # ç´¯ç§¯å˜æ›´ä¿¡æ¯
            accumulated_changes.append({
                'commit_index': current_commit_index,
                'commit_message': commits[current_commit_index - 1].message.strip() if current_commit_index > 0 else "Initial commit",
                'changes': detailed_changes
            })
            print("llmè°ƒç”¨å‰æ£€æŸ¥",accumulated_changes)
            # è¯„ä¼°å½“å‰ç´¯ç§¯çš„å˜æ›´æ˜¯å¦è¶³å¤Ÿ
            evaluation_result = self._evaluate_changes_with_llm(
                accumulated_changes, project_name, language, use_cache
            )
            print("è¯„ä¼°ç»“æœ",evaluation_result)
            
            if evaluation_result['is_worthy']:
                return {
                    "commits_to_check":commits_to_check,
                    'is_worthy': True,
                    'final_commit_index': current_commit_index,
                    'accumulated_changes': accumulated_changes,
                    'evaluation': evaluation_result,
                    'commits_processed': commits_to_check + 1
                }
            else:
                commits_to_check+=1
                return {
                "commits_to_check":commits_to_check,
                'is_worthy': False,
                'final_commit_index': current_commit_index,
                'accumulated_changes': accumulated_changes,
                'evaluation': evaluation_result if 'evaluation_result' in locals() else {'reason': 'è¾¾åˆ°æœ€å¤§æ£€æŸ¥æäº¤æ•°é‡'},
                'commits_processed': commits_to_check
            }
        print("åˆ°è¾¾è¿ç»­æ¬¡æ•°",commits_to_check) 
        return {
                    'is_worthy': True,
                    'final_commit_index': current_commit_index,
                    'accumulated_changes': accumulated_changes,
                    'commits_to_check':commits_to_check,
                    'evaluation': {'reason': 'è¾¾åˆ°æœ€å¤§æ£€æŸ¥æäº¤æ•°é‡'},
                    'commits_processed': commits_to_check + 1
                }
        # å¦‚æœæ£€æŸ¥äº†æœ€å¤§æ•°é‡çš„æäº¤ä»ä¸å¤Ÿï¼Œè¿”å›å½“å‰ç´¯ç§¯çš„å†…å®¹
        
    
    def _evaluate_changes_with_llm(self, accumulated_changes, project_name, language, use_cache):
        """ä½¿ç”¨LLMè¯„ä¼°ç´¯ç§¯çš„å˜æ›´æ˜¯å¦å€¼å¾—ä½œä¸ºä¸€ä¸ªå…³å¡"""
        
        # æ„å»ºå˜æ›´æ‘˜è¦
        changes_summary = []
        total_additions = 0
        total_deletions = 0
        meaningful_files = set()
        
        for change_info in accumulated_changes:
            commit_msg = change_info['commit_message']
            changes = change_info['changes']
            
            changes_summary.append(f"æäº¤ {change_info['commit_index']}: {commit_msg}")
            
            for file_change in changes.get('file_changes', []):
                if file_change.get('diff_content') and file_change['diff_content'] != "[Binary file diff]":
                    # ç»Ÿè®¡ä»£ç è¡Œæ•°å˜æ›´
                    diff_lines = file_change['diff_content'].split('\n')
                    additions = len([line for line in diff_lines if line.startswith('+')])
                    deletions = len([line for line in diff_lines if line.startswith('-')])
                    
                    total_additions += additions
                    total_deletions += deletions
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„æ–‡ä»¶ï¼ˆéé…ç½®æ–‡ä»¶ã€éREADMEç­‰ï¼‰
                    # file_path = file_change['path'].lower()
                    # if any(ext in file_path for ext in ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.sol']):
                    meaningful_files.add(file_change['path'])
                    
                    changes_summary.append(f"  - {file_change['path']} ({file_change['type']}): +{additions}/-{deletions}")
        
        changes_text = '\n'.join(changes_summary)
        print(changes_text)
        # æ„å»ºLLMæç¤ºè¯
        prompt = f"""
è¯·è¯„ä¼°é¡¹ç›® `{project_name}` çš„ä»¥ä¸‹ä»£ç å˜æ›´æ˜¯å¦å€¼å¾—ä½œä¸ºä¸€ä¸ªå…¥é—¨ç¼–ç¨‹å­¦ä¹ å…³å¡ï¼š
åœ¨æ›´æ”¹ä»£ç å¾ˆå°‘çš„æ—¶å€™æ€è€ƒæ˜¯å¦åœ¨ä»‹ç»åŸºç¡€çŸ¥è¯†è¯­æ³•
## å˜æ›´æ‘˜è¦
{changes_text}

## ç»Ÿè®¡ä¿¡æ¯
- æ€»ä»£ç è¡Œæ•°å˜æ›´: +{total_additions}/-{total_deletions}
- ç´¯ç§¯æäº¤æ•°: {len(accumulated_changes)}

## è¯„ä¼°æ ‡å‡†
ä¸€ä¸ªå€¼å¾—çš„å…³å¡åº”è¯¥æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼š
1. å¼•å…¥äº†æ–°çš„ç¼–ç¨‹æ¦‚å¿µæˆ–æŠ€æœ¯ç‚¹å°±é€šè¿‡æ¯”å¦‚åˆåŒçš„åˆ›å»ºï¼ŒçŠ¶æ€å˜é‡å’Œæ•´æ•°ï¼Œæ•°å­¦è¿ç®—
2. éœ€è¦ä½œä¸ºä¸ºæ–°äººè®²è§£è¿™ä¸ªçŸ¥è¯†ç‚¹
3. åŒ…å«è¶³å¤Ÿçš„ä»£ç å˜æ›´ï¼ˆé€šå¸¸ > 4è¡Œæœ‰æ•ˆä»£ç ï¼‰
4. æœ‰æ•™å­¦ä»·å€¼ï¼Œèƒ½è®©å­¦ä¹ è€…å­¦åˆ°æ–°çŸ¥è¯†

## ä¸å€¼å¾—çš„æƒ…å†µ
1. ä»…ä»…æ˜¯åˆå§‹åŒ–ç©ºæ–‡ä»¶ï¼ˆå¦‚ç©ºçš„READMEã€.gitignoreç­‰ï¼‰
2. åªæ˜¯ç®€å•çš„é…ç½®ä¿®æ”¹
3. ä»£ç å˜æ›´è¿‡å°‘ï¼Œæ²¡æœ‰å®è´¨å†…å®¹
4. é‡å¤æ€§çš„ç®€å•æ“ä½œ

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
```json
{{
    "is_worthy": true/false,
    "confidence": 0.0-1.0,
    "reason": "è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆå€¼å¾—æˆ–ä¸å€¼å¾—ä½œä¸ºå…³å¡",
    "key_concepts": ["å¦‚æœå€¼å¾—ï¼Œåˆ—å‡ºä¸»è¦çš„å­¦ä¹ æ¦‚å¿µ"],
    "suggestions": "å¦‚æœä¸å€¼å¾—ï¼Œå»ºè®®ç­‰å¾…ä»€ä¹ˆæ ·çš„å˜æ›´"
}}
```
"""
        
        try:
            response = call_llm(prompt, use_cache=use_cache)
            
            # è§£æJSONå“åº”
            import json
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response.strip()
            
            result = json.loads(json_str)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if 'is_worthy' not in result:
                result['is_worthy'] = total_additions + total_deletions > self.min_code_changes
                result['reason'] = "LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™åˆ¤æ–­"

            return result
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™
            is_worthy = (
                total_additions + total_deletions > self.min_code_changes and
                len(meaningful_files) >= self.min_meaningful_files
            )
            
            return {
                'is_worthy': is_worthy,
                'confidence': 0.5,
                'reason': f"LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™: ä»£ç å˜æ›´{total_additions + total_deletions}è¡Œï¼Œæœ‰æ„ä¹‰æ–‡ä»¶{len(meaningful_files)}ä¸ª",
                'error': str(e)
            }
    
    def post(self, shared, prep_res, exec_res):
        # æ›´æ–°å…±äº«æ•°æ®
        shared["context_evaluation"] = exec_res
        shared["currentIndex"] = exec_res["final_commit_index"]
        shared["commits_to_check"] = exec_res["commits_to_check"]
        shared["accumulated_changes"] = exec_res["accumulated_changes"]
        commits_to_check =shared["commits_to_check"]
        if exec_res['is_worthy']:
            print(f"âœ… {commits_to_check}å½“å‰ä¸Šä¸‹æ–‡å€¼å¾—ä½œä¸ºå…³å¡")
            print(f"ğŸ“ è¯„ä¼°åŸå› : {exec_res['evaluation']['reason']}")
            return "worthy"
        else:
            print(f"âŒ {commits_to_check}å½“å‰ä¸Šä¸‹æ–‡ä¸å€¼å¾—ä½œä¸ºå…³å¡")
            print(f"ğŸ“ è¯„ä¼°åŸå› : {exec_res['evaluation']['reason']}")
            return "not_worthy"


#æ•™ç¨‹ç”ŸæˆèŠ‚ç‚¹ï¼ŒçŸ¥è¯†ç‚¹çš„è®²è§£      
class KnowledgePointAnalysis(Node):
    def prep(self, shared):
        print("ç°åœ¨æ˜¯KnowledgePointAnalysisèŠ‚ç‚¹")
        repo = shared["repo"]
        currentIndex = shared["currentIndex"]
        fullcommits = shared["fullcommits"]
        project_name = shared["project_name"]
        accumulated_changes = shared["accumulated_changes"]
        knowledge = shared["knowledge"]
        max_commits_to_check = shared["max_commits_to_check"]
        language = shared.get("language", "chinese")
        use_cache = shared.get("use_cache", True) 
        return (fullcommits,accumulated_changes,repo, currentIndex,max_commits_to_check,knowledge, project_name, language, use_cache)
    
    def exec(self, prep_res):
        (commits_to_check,fullcommits,accumulated_changes,repo,currentIndex,max_commits_to_check, knowledge,project_name, language, use_cache) = prep_res
        
        current_commit_index = currentIndex
        
        print("æ›´æ”¹",accumulated_changes)
        commits = list(fullcommits)
        print("commitsçš„é•¿åº¦",len(commits))
        print(f'è¿ç»­è¯»å–æ¬¡æ•°{commits_to_check},æœ€å¤§è¯»å–æ¬¡æ•°{max_commits_to_check}')
        if commits_to_check <= max_commits_to_check:
            # if current_commit_index >= len(commits):
            #     break
                
            # è·å–å½“å‰æäº¤çš„è¯¦ç»†å˜æ›´
            detailed_changes = get_commit_changes_detailed(repo, current_commit_index, include_diff_content=True)
            
            # ç´¯ç§¯å˜æ›´ä¿¡æ¯
            accumulated_changes.append({
                'commit_index': current_commit_index,
                'commit_message': commits[current_commit_index - 1].message.strip() if current_commit_index > 0 else "Initial commit",
                'changes': detailed_changes
            })
            print("llmè°ƒç”¨å‰æ£€æŸ¥",accumulated_changes)
            # è¯„ä¼°å½“å‰ç´¯ç§¯çš„å˜æ›´æ˜¯å¦è¶³å¤Ÿ
            evaluation_result = self._evaluate_changes_with_llm(
                accumulated_changes, project_name, language, use_cache
            )
            print("è¯„ä¼°ç»“æœ",evaluation_result)
            
            if evaluation_result['is_worthy']:
                return {
                    "commits_to_check":commits_to_check,
                    'is_worthy': True,
                    'final_commit_index': current_commit_index,
                    'accumulated_changes': accumulated_changes,
                    'evaluation': evaluation_result,
                    'commits_processed': commits_to_check + 1
                }
            else:
                commits_to_check+=1
                return {
                "commits_to_check":commits_to_check,
                'is_worthy': False,
                'final_commit_index': current_commit_index,
                'accumulated_changes': accumulated_changes,
                'evaluation': evaluation_result if 'evaluation_result' in locals() else {'reason': 'è¾¾åˆ°æœ€å¤§æ£€æŸ¥æäº¤æ•°é‡'},
                'commits_processed': commits_to_check
            }
        print("åˆ°è¾¾è¿ç»­æ¬¡æ•°",commits_to_check) 
        return {
                    'is_worthy': True,
                    'final_commit_index': current_commit_index,
                    'accumulated_changes': accumulated_changes,
                    'commits_to_check':commits_to_check,
                    'evaluation': {'reason': 'è¾¾åˆ°æœ€å¤§æ£€æŸ¥æäº¤æ•°é‡'},
                    'commits_processed': commits_to_check + 1
                }
        # å¦‚æœæ£€æŸ¥äº†æœ€å¤§æ•°é‡çš„æäº¤ä»ä¸å¤Ÿï¼Œè¿”å›å½“å‰ç´¯ç§¯çš„å†…å®¹
        
    
    def _evaluate_changes_with_llm(self, accumulated_changes, project_name, language, use_cache):
        """ä½¿ç”¨LLMè¯„ä¼°ç´¯ç§¯çš„å˜æ›´æ˜¯å¦å€¼å¾—ä½œä¸ºä¸€ä¸ªå…³å¡"""
        
        # æ„å»ºå˜æ›´æ‘˜è¦
        changes_summary = []
        total_additions = 0
        total_deletions = 0
        meaningful_files = set()
        
        for change_info in accumulated_changes:
            commit_msg = change_info['commit_message']
            changes = change_info['changes']
            
            changes_summary.append(f"æäº¤ {change_info['commit_index']}: {commit_msg}")
            
            for file_change in changes.get('file_changes', []):
                if file_change.get('diff_content') and file_change['diff_content'] != "[Binary file diff]":
                    # ç»Ÿè®¡ä»£ç è¡Œæ•°å˜æ›´
                    diff_lines = file_change['diff_content'].split('\n')
                    additions = len([line for line in diff_lines if line.startswith('+')])
                    deletions = len([line for line in diff_lines if line.startswith('-')])
                    
                    total_additions += additions
                    total_deletions += deletions
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„æ–‡ä»¶ï¼ˆéé…ç½®æ–‡ä»¶ã€éREADMEç­‰ï¼‰
                    # file_path = file_change['path'].lower()
                    # if any(ext in file_path for ext in ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.sol']):
                    meaningful_files.add(file_change['path'])
                    
                    changes_summary.append(f"  - {file_change['path']} ({file_change['type']}): +{additions}/-{deletions}")
        
        changes_text = '\n'.join(changes_summary)
        print(changes_text)
        # æ„å»ºLLMæç¤ºè¯
        prompt = f"""
è¯·è¯„ä¼°é¡¹ç›® `{project_name}` çš„ä»¥ä¸‹ä»£ç å˜æ›´æ˜¯å¦å€¼å¾—ä½œä¸ºä¸€ä¸ªå…¥é—¨ç¼–ç¨‹å­¦ä¹ å…³å¡ï¼š
åœ¨æ›´æ”¹ä»£ç å¾ˆå°‘çš„æ—¶å€™æ€è€ƒæ˜¯å¦åœ¨ä»‹ç»åŸºç¡€çŸ¥è¯†è¯­æ³•
## å˜æ›´æ‘˜è¦
{changes_text}

## ç»Ÿè®¡ä¿¡æ¯
- æ€»ä»£ç è¡Œæ•°å˜æ›´: +{total_additions}/-{total_deletions}
- ç´¯ç§¯æäº¤æ•°: {len(accumulated_changes)}

## è¯„ä¼°æ ‡å‡†
ä¸€ä¸ªå€¼å¾—çš„å…³å¡åº”è¯¥æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼š
1. å¼•å…¥äº†æ–°çš„ç¼–ç¨‹æ¦‚å¿µæˆ–æŠ€æœ¯ç‚¹å°±é€šè¿‡æ¯”å¦‚åˆåŒçš„åˆ›å»ºï¼ŒçŠ¶æ€å˜é‡å’Œæ•´æ•°ï¼Œæ•°å­¦è¿ç®—
2. éœ€è¦ä½œä¸ºä¸ºæ–°äººè®²è§£è¿™ä¸ªçŸ¥è¯†ç‚¹
3. åŒ…å«è¶³å¤Ÿçš„ä»£ç å˜æ›´ï¼ˆé€šå¸¸ > 4è¡Œæœ‰æ•ˆä»£ç ï¼‰
4. æœ‰æ•™å­¦ä»·å€¼ï¼Œèƒ½è®©å­¦ä¹ è€…å­¦åˆ°æ–°çŸ¥è¯†

## ä¸å€¼å¾—çš„æƒ…å†µ
1. ä»…ä»…æ˜¯åˆå§‹åŒ–ç©ºæ–‡ä»¶ï¼ˆå¦‚ç©ºçš„READMEã€.gitignoreç­‰ï¼‰
2. åªæ˜¯ç®€å•çš„é…ç½®ä¿®æ”¹
3. ä»£ç å˜æ›´è¿‡å°‘ï¼Œæ²¡æœ‰å®è´¨å†…å®¹
4. é‡å¤æ€§çš„ç®€å•æ“ä½œ

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
```json
{{
    "is_worthy": true/false,
    "confidence": 0.0-1.0,
    "reason": "è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆå€¼å¾—æˆ–ä¸å€¼å¾—ä½œä¸ºå…³å¡",
    "key_concepts": ["å¦‚æœå€¼å¾—ï¼Œåˆ—å‡ºä¸»è¦çš„å­¦ä¹ æ¦‚å¿µ"],
    "suggestions": "å¦‚æœä¸å€¼å¾—ï¼Œå»ºè®®ç­‰å¾…ä»€ä¹ˆæ ·çš„å˜æ›´"
}}
```
"""
        
        try:
            response = call_llm(prompt, use_cache=use_cache)
            
            # è§£æJSONå“åº”
            import json
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response.strip()
            
            result = json.loads(json_str)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if 'is_worthy' not in result:
                result['is_worthy'] = total_additions + total_deletions > self.min_code_changes
                result['reason'] = "LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™åˆ¤æ–­"

            return result
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™
            is_worthy = (
                total_additions + total_deletions > self.min_code_changes and
                len(meaningful_files) >= self.min_meaningful_files
            )
            
            return {
                'is_worthy': is_worthy,
                'confidence': 0.5,
                'reason': f"LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™: ä»£ç å˜æ›´{total_additions + total_deletions}è¡Œï¼Œæœ‰æ„ä¹‰æ–‡ä»¶{len(meaningful_files)}ä¸ª",
                'error': str(e)
            }
    
    def post(self, shared, prep_res, exec_res):
        # æ›´æ–°å…±äº«æ•°æ®
        shared["context_evaluation"] = exec_res
        shared["currentIndex"] = exec_res["final_commit_index"]
        shared["commits_to_check"] = exec_res["commits_to_check"]
        shared["accumulated_changes"] = exec_res["accumulated_changes"]
        commits_to_check =shared["commits_to_check"]
        if exec_res['is_worthy']:
            print(f"âœ… {commits_to_check}å½“å‰ä¸Šä¸‹æ–‡å€¼å¾—ä½œä¸ºå…³å¡")
            print(f"ğŸ“ è¯„ä¼°åŸå› : {exec_res['evaluation']['reason']}")
            return "worthy"
        else:
            print(f"âŒ {commits_to_check}å½“å‰ä¸Šä¸‹æ–‡ä¸å€¼å¾—ä½œä¸ºå…³å¡")
            print(f"ğŸ“ è¯„ä¼°åŸå› : {exec_res['evaluation']['reason']}")
            return "not_worthy"



class ToLevelConverter(Node):
    def prep(self, shared):
        print("ç°åœ¨æ˜¯ç”ŸæˆèŠ‚ç‚¹")
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True
        files_data = shared["files"]  # è·å–æ–‡ä»¶æ•°æ®
        currentIndex = shared["currentIndex"]  # è·å–æ–‡ä»¶æ•°æ®
        project_name = shared["project_name"]  # ä»å…±äº«æ•°æ®è·å–é¡¹ç›®åç§°
        repo = shared["repo"]  # ä»å…±äº«æ•°æ®è·å–é¡¹ç›®åç§°
        knowledge = shared["knowledge"]  # ä»å…±äº«æ•°æ®è·å–é¡¹ç›®åç§°
        language = shared.get("language", "chinese")
        
        language_instruction = ""
        name_lang_hint = ""
        desc_lang_hint = ""
        if language.lower() != "english":
            # éè‹±è¯­è¾“å‡ºæ—¶éœ€è¦ç‰¹æ®Šæ ‡è®°
            language_instruction = f"IMPORTANT: Generate the `name` and `description` in **{language.capitalize()}**\n\n"
            name_lang_hint = f" ({language} output)"
            desc_lang_hint = f" ({language} output)"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡è¯„ä¼°ç»“æœï¼Œå¦‚æœæœ‰åˆ™ä½¿ç”¨ç´¯ç§¯çš„å˜æ›´
        context_evaluation = shared.get("context_evaluation")
        if context_evaluation and context_evaluation.get("accumulated_changes"):
            # ä½¿ç”¨ç´¯ç§¯çš„å˜æ›´ä¿¡æ¯
            buffer = []
            buffer.append("\nç´¯ç§¯æ–‡ä»¶å˜åŒ–è¯¦æƒ…:")
            
            for change_info in context_evaluation["accumulated_changes"]:
                commit_index = change_info["commit_index"]
                commit_msg = change_info["commit_message"]
                changes = change_info["changes"]
                
                buffer.append(f"\n=== æäº¤ {commit_index}: {commit_msg} ===")
                
                for i, file_change in enumerate(changes.get('file_changes', [])):
                    if file_change.get('diff_content') and file_change['diff_content'] != "[Binary file diff]":
                        diff_lines = file_change['diff_content'].split('\n')
                        # æˆªæ–­diffå†…å®¹ä»¥æ§åˆ¶é•¿åº¦
                        truncated_diff_lines = token_manager.truncate_diff_content(diff_lines, max_lines=50)
                        
                        buffer.append(f"  {i+1}. {file_change['path']} ({file_change['type']})")
                        buffer.append(f"     Diffå†…å®¹:")
                        for line in truncated_diff_lines:
                            if line.startswith('+'):
                                buffer.append(f"       {line}")
                            elif line.startswith('-'):
                                buffer.append(f"       {line}")
                            elif line.startswith('@@'):
                                buffer.append(f"       {line}")
                            elif line.startswith('[...'):
                                buffer.append(f"       {line}")
        else:
            # åŸæœ‰çš„å•ä¸ªæäº¤å¤„ç†é€»è¾‘
            print(f"urlï¼š{project_name} å½“å‰å…³å¡ï¼š{currentIndex}")
            detailed_changes = get_commit_changes_detailed(repo, currentIndex, include_diff_content=True)
            buffer = []
            buffer.append("\næ–‡ä»¶å˜åŒ–è¯¦æƒ…:")
            print(detailed_changes)
            for i, file_change in enumerate(detailed_changes['file_changes']):  # æ˜¾ç¤ºæ‰€æœ‰æ–‡ä»¶
                # æ˜¾ç¤ºdiffå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if file_change.get('diff_content') and file_change['diff_content'] != "[Binary file diff]":
                    diff_lines = file_change['diff_content'].split('\n')[:]
                    # æˆªæ–­diffå†…å®¹ä»¥æ§åˆ¶é•¿åº¦
                    truncated_diff_lines = token_manager.truncate_diff_content(diff_lines, max_lines=50)
                    
                    buffer.append(f"     Diffå†…å®¹:")
                    buffer.append(f"  {i+1}. {file_change['path']} ({file_change['type']})")
                    for line in truncated_diff_lines:
                        if line.startswith('+'):
                            buffer.append(f"       {line}")
                        elif line.startswith('-'):
                            buffer.append(f"       {line}")
                        elif line.startswith('@@'):
                            buffer.append(f"       {line}")
                        elif line.startswith('[...'):
                            buffer.append(f"       {line}")
        
        buffer = '\n'.join(buffer)
        shared["diff"] = buffer
        print(buffer)
        return (
            buffer,
            language_instruction,
            desc_lang_hint,
            name_lang_hint,
            files_data,
            knowledge,
            use_cache,
            project_name,
            language,
        )
        
    def exec(self, prep_res):
        (
            buffer,
            language_instruction,
            desc_lang_hint,
            name_lang_hint,
            files_data,
            knowledge,
            use_cache,
            project_name,
            language,
        ) = prep_res  # Unpack parameters
        prompt = f"""
è¯·æ ¹æ®é¡¹ç›® `{project_name}` çš„ä»£ç åº“è®¾è®¡ç¼–ç¨‹å­¦ä¹ å…³å¡ï¼Œå…³å¡æè¿°ä½¿ç”¨markdownè¾“å‡ºï¼š

ä»£ç å˜æ›´è¯¦æƒ…ï¼š
{buffer}
æ•™ç¨‹è§„èŒƒ
1. **å†…å®¹è¯´æ˜** - ç”¨æ˜“äºç†è§£çš„è¯æè¿°
2. **è¯­æ³•è®²è§£** - ä½¿ç”¨ä¾‹å­è®²è§£è¯­æ³•
3. **æŠŠç”¨æˆ·å½“æˆç™½ç—´** - å°½å¯èƒ½æ•™ä¼šä»–ä»¬

è¾“å‡ºå­—æ®µè¯´æ˜:
    "name":æ ‡é¢˜åç§°ï¼Œ
    "description":æ•™ç¨‹ï¼Œ
    "requirements":ç¼–ç¨‹æŒ‘æˆ˜ï¼Œå¤åˆ»ä»£ç ï¼Œæ³¨æ„ä¸¾å¾—ä¾‹å­ä¸è¦æ˜¯ç­”æ¡ˆ!

Format the output as YAML:
```yaml
  name: æ•°ç»„åŸºç¡€è¯­æ³•
  description: |-
    å¦‚æœä½ æƒ³å»ºç«‹ä¸€ä¸ªé›†åˆï¼Œå¯ä»¥ç”¨ _æ•°ç»„_ è¿™æ ·çš„æ•°æ®ç±»å‹ã€‚Solidity æ”¯æŒä¸¤ç§æ•°ç»„: _é™æ€_ æ•°ç»„å’Œ _åŠ¨æ€_ æ•°ç»„:
    ```solidity
    // å›ºå®šé•¿åº¦ä¸º2çš„é™æ€æ•°ç»„:
    uint[2] fixedArray;
    // å›ºå®šé•¿åº¦ä¸º5çš„stringç±»å‹çš„é™æ€æ•°ç»„:
    string[5] stringArray;
    // åŠ¨æ€æ•°ç»„ï¼Œé•¿åº¦ä¸å›ºå®šï¼Œå¯ä»¥åŠ¨æ€æ·»åŠ å…ƒç´ :
    uint[] dynamicArray;
    ```
    ä½ ä¹Ÿå¯ä»¥å»ºç«‹ä¸€ä¸ªç»“æ„ä½“ç±»å‹çš„æ•°ç»„ï¼Œä¾‹å¦‚ï¼Œä¸Šä¸€ç« æåˆ°çš„ Person:
    ```solidity
    Person[] people; // è¿™æ˜¯åŠ¨æ€æ•°ç»„ï¼Œæˆ‘ä»¬å¯ä»¥ä¸æ–­æ·»åŠ å…ƒç´ 
    ```
    
    ## å…¬å…±æ•°ç»„
    ä½ å¯ä»¥å®šä¹‰ public æ•°ç»„ï¼ŒSolidity ä¼šè‡ªåŠ¨åˆ›å»º getter æ–¹æ³•ã€‚è¯­æ³•å¦‚ä¸‹:
    ```solidity
    Person[] public people;
    ```
    å…¶å®ƒçš„åˆçº¦å¯ä»¥ä»è¿™ä¸ªæ•°ç»„è¯»å–æ•°æ®ï¼ˆä½†ä¸èƒ½å†™å…¥æ•°æ®ï¼‰ï¼Œæ‰€ä»¥è¿™åœ¨åˆçº¦ä¸­æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ä¿å­˜å…¬å…±æ•°æ®çš„æ¨¡å¼ã€‚
    
  requirements: |
    ä¸ºäº†æŠŠä¸€ä¸ªåƒµå°¸éƒ¨é˜Ÿä¿å­˜åœ¨æˆ‘ä»¬çš„APPé‡Œï¼Œå¹¶ä¸”èƒ½å¤Ÿè®©å…¶å®ƒAPPçœ‹åˆ°è¿™äº›åƒµå°¸ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå…¬å…±æ•°ç»„ã€‚
    åˆ›å»ºä¸€ä¸ªæ•°æ®ç±»å‹ä¸º Zombie çš„ç»“æ„ä½“æ•°ç»„ï¼Œç”¨ public ä¿®é¥°ï¼Œå‘½åä¸ºï¼šzombiesã€‚
```

Now, provide the YAML output:
"""  
        # ä¼˜åŒ–promptä»¥æ§åˆ¶tokenæ•°é‡
        optimized_buffer, optimized_prompt = token_manager.optimize_prompt_for_level_generation(
            buffer, prompt, max_buffer_tokens=80000
        )
        
        # é‡æ–°æ„å»ºæœ€ç»ˆprompt
        final_prompt = prompt.replace(buffer, optimized_buffer)
        
        logger.info(f"1. ä»£ç å˜æ›´è¯¦æƒ…tokenæ•°: {token_manager.count_tokens(optimized_buffer)} 2. å…¨å±€çŸ¥è¯†ç‚¹ï¼š{knowledge}")
     
        response = safe_call_llm(final_prompt, use_cache)
        # --- Validation ---
        print(response)
        # yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        # Level_raw = yaml.safe_load(yaml_str)
        # print(Level_raw)
        Level_raw = robust_yaml_parse(response)
        print(Level_raw)
        
        if not isinstance(Level_raw, list):
            print("llm oup is not a list")
            return [
                {
                    'name': 'ç©ºç™½',
                    'description': 'ç©ºç™½', 
                    'requirements': 'ç©ºç™½'}]
        # print("__________________è¾“å‡º_______________________")
        # print(Level_raw)
        return Level_raw
    
    def post(self, shared, prep_res, exec_res):
        shared["currentIndex"] += 1
        print(shared["currentIndex"])
        shared["res"] = exec_res
        return


class SkipToNextCommitNode(Node):
    """è·³è½¬åˆ°ä¸‹ä¸€ä¸ªæäº¤çš„èŠ‚ç‚¹ï¼Œç”¨äºå½“å‰æäº¤ä¸å€¼å¾—ä½œä¸ºå…³å¡æ—¶"""
    
    def prep(self, shared):
        
        currentIndex = shared["currentIndex"]
        fullcommits = shared["fullcommits"]
        print("ç°åœ¨æ˜¯skipèŠ‚ç‚¹",currentIndex)
        repo = shared["repo"]
        
        return currentIndex, repo,fullcommits
    
    def exec(self, prep_res):
        currentIndex, repo , fullcommits = prep_res
        
        commits = list(fullcommits)
        next_index = currentIndex + 1
        
        if next_index >= len(commits):
            return {
                "has_next": False,
                "message": "å·²åˆ°è¾¾æœ€åä¸€ä¸ªæäº¤",
                "next_index": currentIndex
            }
        
        # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæäº¤
        checkout_to_commit(repo, next_index)
        
        return {
            "has_next": True,
            "message": f"å·²è·³è½¬åˆ°æäº¤ {next_index}",
            "next_index": next_index
        }
    
    def post(self, shared, prep_res, exec_res):
        if exec_res["has_next"]:
            shared["currentIndex"] = exec_res["next_index"]
            print(f"ğŸ”„ {exec_res['message']}")
            return "continue"
        else:
            print(f"â¹ï¸ {exec_res['message']}")
            return "end"


class GetLevelInfoNode(Node):
    """è·å–å…³å¡ä¿¡æ¯èŠ‚ç‚¹ï¼šä»æ•°æ®åº“è·å–å…³å¡è¦æ±‚å’Œæè¿°"""
    
    def __init__(self):
        super().__init__()
        self.cur_retry = 0
    
    def prep(self, shared):
        level_id = shared.get("level_id")
        course_id = shared.get("course_id")
        
        if not level_id or not course_id:
            raise ValueError("ç¼ºå°‘å¿…è¦å‚æ•°ï¼šlevel_id å’Œ course_id")
        
        return level_id, course_id
    
    def exec(self, prep_res):
        level_id, course_id = prep_res
        
        try:
            # è¿™é‡Œéœ€è¦å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—
            from app.database.connection import SessionLocal
            from app.models.level import Level
            from app.models.course import Course
            
            db = SessionLocal()
            try:
                # è·å–å…³å¡ä¿¡æ¯
                level = db.query(Level).filter(
                    Level.order_number == level_id,
                    Level.course_id == course_id
                ).first()
                
                if not level:
                    raise ValueError(f"æœªæ‰¾åˆ°å…³å¡ {level_id}")
                
                # è·å–è¯¾ç¨‹ä¿¡æ¯
                course = db.query(Course).filter(Course.id == course_id).first()
                if not course:
                    raise ValueError(f"æœªæ‰¾åˆ°è¯¾ç¨‹ {course_id}")
                
                return {
                    "level": {
                        "id": level.id,
                        "title": level.title,
                        "description": level.description,
                        "requirements": level.requirements,
                        "order_number": level.order_number
                    },
                    "course": {
                        "id": course.id,
                        "title": course.title,
                        "git_url": course.git_url
                    }
                }
                
            finally:
                db.close()
                
        except Exception as e:
            raise Exception(f"è·å–å…³å¡ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def post(self, shared, prep_res, exec_res):
        shared["level_info"] = exec_res["level"]
        shared["course_info"] = exec_res["course"]
        return "default"


class CloneRepoNode(Node):
    """å…‹éš†ä»“åº“èŠ‚ç‚¹ï¼šå…‹éš†è¯¾ç¨‹ä»“åº“å¹¶é‡ç½®åˆ°æŒ‡å®šæäº¤"""
    
    def prep(self, shared):
        course_info = shared.get("course_info")
        level_info = shared.get("level_info")
        
        if not course_info or not level_info:
            raise ValueError("ç¼ºå°‘è¯¾ç¨‹æˆ–å…³å¡ä¿¡æ¯")
        
        git_url = course_info["git_url"]
        order_number = level_info["order_number"]
        
        # è®¡ç®—æäº¤ç´¢å¼•ï¼ˆå…³å¡é¡ºåºå· + 1ï¼Œå› ä¸ºä»ç¬¬2ä¸ªæäº¤å¼€å§‹ï¼‰
        commit_index = order_number + 1
        
        return git_url, commit_index
    
    def exec(self, prep_res):
        git_url, commit_index = prep_res
        
        try:
            # ä½¿ç”¨å…±äº«ç›®å½•è·å–æˆ–å…‹éš†ä»“åº“
            repo = get_or_clone_repository(git_url,update_to_latest=False)
            
            # è·å–æ‰€æœ‰æäº¤
            commits = list(repo.iter_commits(reverse=True))
            
            # éªŒè¯å¹¶è°ƒæ•´æäº¤ç´¢å¼•
            if commit_index > len(commits):
                print(f"è­¦å‘Š: æäº¤ç´¢å¼• {commit_index} è¶…å‡ºèŒƒå›´ï¼Œä»“åº“åªæœ‰ {len(commits)} ä¸ªæäº¤ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæäº¤")
                commit_index = len(commits)
            elif commit_index < 1:
                print(f"è­¦å‘Š: æäº¤ç´¢å¼• {commit_index} æ— æ•ˆï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæäº¤")
                commit_index = 1
            
            # åˆ‡æ¢åˆ°æŒ‡å®šæäº¤
            checkout_to_commit(repo, commit_index)
            
            return {
                "repo": repo,
                "tmpdirname": repo.working_dir,
                "commits": commits,
                "commit_index": commit_index
            }
            
        except Exception as e:
            raise Exception(f"å…‹éš†ä»“åº“å¤±è´¥: {str(e)}")
    
    def post(self, shared, prep_res, exec_res):
        # å°†ä»“åº“ä¿¡æ¯å­˜å‚¨åˆ°sharedä¸­ï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨
        shared["repo_info"] = exec_res
        shared["repo"] = exec_res["repo"]
        shared["tmpdirname"] = exec_res["tmpdirname"]
        shared["commits"] = exec_res["commits"]
        shared["commit_index"] = exec_res["commit_index"]
        return "default"


class GetStandardCodeNode(Node):
    """è·å–æ ‡å‡†ç­”æ¡ˆä»£ç èŠ‚ç‚¹ï¼šä»æŒ‡å®šæäº¤è·å–æ ‡å‡†å®ç°ä»£ç """
    
    def prep(self, shared):
        repo_info = shared.get("repo_info")
        if not repo_info:
            raise ValueError("ç¼ºå°‘ä»“åº“ä¿¡æ¯")
        
        tmpdirname = repo_info["tmpdirname"]
        return tmpdirname
    
    def exec(self, prep_res):
        tmpdirname = prep_res
        
        try:
            # è¯»å–å½“å‰æäº¤çš„æ‰€æœ‰ä»£ç æ–‡ä»¶
            result = filter_and_read_files(
                tmpdirname,
                max_file_size=1 * 1024 * 1024,
                include_patterns=get_file_patterns("code"),
                exclude_patterns=get_exclude_patterns("common")
            )
            
            return result["files"]
            
        except Exception as e:
            raise Exception(f"è·å–æ ‡å‡†ä»£ç å¤±è´¥: {str(e)}")
    
    def post(self, shared, prep_res, exec_res):
        shared["standard_code"] = exec_res
        return "default"


class AnalyzeUserCodeNode(Node):
    """åˆ†æç”¨æˆ·ä»£ç èŠ‚ç‚¹ï¼šè§£æç”¨æˆ·æäº¤çš„æ–‡ä»¶æ ‘ç»“æ„å’Œä»£ç å†…å®¹"""
    
    def prep(self, shared):
        user_file_tree = shared.get("user_file_tree")
        if not user_file_tree:
            raise ValueError("ç¼ºå°‘ç”¨æˆ·æ–‡ä»¶æ ‘æ•°æ®")
        
        return user_file_tree
    
    def exec(self, prep_res):
        user_file_tree = prep_res
        
        try:
            # è§£ææ–‡ä»¶æ ‘ï¼Œæå–æ–‡ä»¶å†…å®¹
            user_files = {}
            
            def extract_files(node, current_path=""):
                if node.get("type") == "file":
                    file_path = node.get("uri", "").replace("file://", "")
                    if current_path:
                        relative_path = file_path.replace(current_path, "").lstrip("/")
                    else:
                        relative_path = file_path.split("/")[-1]  # åªå–æ–‡ä»¶å
                    
                    content = node.get("content", "")
                    if content:
                        user_files[relative_path] = content
                
                elif node.get("type") == "directory":
                    children = node.get("children", [])
                    for child in children:
                        extract_files(child, current_path)
            
            extract_files(user_file_tree)
            
            return user_files
            
        except Exception as e:
            raise Exception(f"åˆ†æç”¨æˆ·ä»£ç å¤±è´¥: {str(e)}")
    
    def post(self, shared, prep_res, exec_res):
        shared["user_code"] = exec_res
        return "default"


class CompareAndJudgeNode(Node):
    """å¯¹æ¯”åˆ¤æ–­èŠ‚ç‚¹ï¼šä½¿ç”¨LLMå¯¹æ¯”ç”¨æˆ·ä»£ç å’Œæ ‡å‡†ç­”æ¡ˆï¼Œç»™å‡ºè¯„åˆ¤ç»“æœ"""
    
    def __init__(self):
        super().__init__()
        self.cur_retry = 0  # æ·»åŠ é‡è¯•è®¡æ•°å™¨
    
    def prep(self, shared):
        level_info = shared.get("level_info")
        standard_code = shared.get("standard_code")
        user_code = shared.get("user_code")
        
        if not all([level_info, standard_code, user_code]):
            raise ValueError("ç¼ºå°‘å¿…è¦çš„å¯¹æ¯”æ•°æ®")
        
        use_cache = shared.get("use_cache", True)
        language = shared.get("language", "chinese")
        
        return level_info, standard_code, user_code, use_cache, language
    
    def exec(self, prep_res):
        level_info, standard_code, user_code, use_cache, language = prep_res
        
        try:
            # æ„å»ºæ ‡å‡†ä»£ç å†…å®¹å­—ç¬¦ä¸²
            standard_code_str = "\n".join([
                f"=== {path} ===\n{content}\n"
                for path, content in standard_code.items()
            ])
            
            # æ„å»ºç”¨æˆ·ä»£ç å†…å®¹å­—ç¬¦ä¸²
            user_code_str = "\n".join([
                f"=== {path} ===\n{content}\n"
                for path, content in user_code.items()
            ])
            
            # æ„å»ºLLMæç¤ºè¯
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹å­¦ä¹ å¹³å°çš„æ™ºèƒ½è¯„åˆ¤ç³»ç»Ÿã€‚è¯·æ ¹æ®å…³å¡è¦æ±‚ï¼Œå¯¹æ¯”ç”¨æˆ·æäº¤çš„ä»£ç å’Œæ ‡å‡†ç­”æ¡ˆï¼Œç»™å‡ºè¯„åˆ¤ç»“æœã€‚

## å…³å¡ä¿¡æ¯
**æ ‡é¢˜**: {level_info['title']}
**æè¿°**: {level_info['description']}
**é€šè¿‡è¦æ±‚**: {level_info['requirements']}

## æ ‡å‡†ç­”æ¡ˆä»£ç 
{standard_code_str}

## ç”¨æˆ·æäº¤ä»£ç 
{user_code_str}

## è¯„åˆ¤è¦æ±‚
è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ¤ï¼š
1. **åŠŸèƒ½å®Œæ•´æ€§**: ç”¨æˆ·ä»£ç æ˜¯å¦å®ç°äº†å…³å¡è¦æ±‚çš„åŠŸèƒ½
2. **ä»£ç æ­£ç¡®æ€§**: è¯­æ³•æ˜¯å¦æ­£ç¡®ï¼Œé€»è¾‘æ˜¯å¦åˆç†
3. **ä»£ç è´¨é‡**: ä»£ç é£æ ¼ã€å‘½åè§„èŒƒç­‰
4. **åˆ›æ–°æ€§**: æ˜¯å¦æœ‰ç‹¬ç‰¹çš„å®ç°æ€è·¯

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¾“å‡ºè¯„åˆ¤ç»“æœï¼š
```json
{{
    "passed": true/false,
    "feedback": "è¯¦ç»†çš„åé¦ˆä¿¡æ¯ï¼ŒåŒ…æ‹¬åšå¾—å¥½çš„åœ°æ–¹å’Œéœ€è¦æ”¹è¿›çš„åœ°æ–¹",
    "suggestions": [
        "å…·ä½“çš„æ”¹è¿›å»ºè®®1",
        "å…·ä½“çš„æ”¹è¿›å»ºè®®2"
    ],
    "praise": "å¦‚æœé€šè¿‡äº†ï¼Œç»™å‡ºé¼“åŠ±æ€§çš„è¯è¯­",
    "detailed_analysis": {{
        "functionality": "åŠŸèƒ½å®Œæ•´æ€§åˆ†æ",
        "correctness": "ä»£ç æ­£ç¡®æ€§åˆ†æ", 
        "quality": "ä»£ç è´¨é‡åˆ†æ",
        "innovation": "åˆ›æ–°æ€§åˆ†æ"
    }}
}}
```

æ³¨æ„ï¼š
- å¦‚æœç”¨æˆ·ä»£ç åŸºæœ¬æ»¡è¶³è¦æ±‚ï¼Œå³ä½¿æœ‰å°é—®é¢˜ä¹Ÿåº”è¯¥ç»™äºˆé€šè¿‡
- åé¦ˆè¦å…·ä½“ã€å»ºè®¾æ€§ï¼Œé¿å…è¿‡äºä¸¥å‰
- å¦‚æœé€šè¿‡äº†è¦ç»™äºˆé¼“åŠ±å’Œè‚¯å®š
"""
            
            print(prompt)
            # è°ƒç”¨LLM
            response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0))
            
            # è§£æJSONå“åº”
            import json
            try:
                # æå–JSONéƒ¨åˆ†
                if "```json" in response:
                    json_str = response.split("```json")[1].split("```")[0].strip()
                else:
                    json_str = response.strip()
                
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_fields = ["passed", "feedback"]
                for field in required_fields:
                    if field not in result:
                        raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                
                return result
                
            except (json.JSONDecodeError, ValueError) as e:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬ç»“æœ
                return {
                    "passed": False,
                    "feedback": f"è¯„åˆ¤ç³»ç»Ÿå‡ºç°é”™è¯¯ï¼Œæ— æ³•è§£æLLMå“åº”: {str(e)}",
                    "suggestions": ["è¯·æ£€æŸ¥ä»£ç æ ¼å¼å’Œè¯­æ³•"],
                    "error": str(e)
                }
                
        except Exception as e:
            return {
                "passed": False,
                "feedback": f"è¯„åˆ¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                "suggestions": ["è¯·ç¨åé‡è¯•"],
                "error": str(e)
            }
    
    def post(self, shared, prep_res, exec_res):
        shared["judgment_result"] = exec_res
        return "default"