#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•tokenç®¡ç†åŠŸèƒ½
"""

from agentflow.utils.call_llm import call_llm
from agentflow.utils.token_manager import token_manager, count_tokens
import tiktoken

def test_token_counting():
    """æµ‹è¯•tokenè®¡æ•°åŠŸèƒ½"""
    print("=== æµ‹è¯•Tokenè®¡æ•°åŠŸèƒ½ ===")
    
    test_text = "Hello, this is a test message for token counting."
    token_count = count_tokens(test_text)
    print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
    print(f"Tokenæ•°é‡: {token_count}")
    
    # éªŒè¯ä¸tiktokençš„ä¸€è‡´æ€§
    encoding = tiktoken.get_encoding("cl100k_base")
    expected_count = len(encoding.encode(test_text))
    print(f"æœŸæœ›Tokenæ•°é‡: {expected_count}")
    print(f"è®¡æ•°æ˜¯å¦ä¸€è‡´: {token_count == expected_count}")
    print()

def test_text_truncation():
    """æµ‹è¯•æ–‡æœ¬æˆªæ–­åŠŸèƒ½"""
    print("=== æµ‹è¯•æ–‡æœ¬æˆªæ–­åŠŸèƒ½ ===")
    
    # åˆ›å»ºä¸€ä¸ªé•¿æ–‡æœ¬
    long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æµ‹è¯•æ–‡æœ¬ã€‚" * 1000
    original_tokens = count_tokens(long_text)
    print(f"åŸå§‹æ–‡æœ¬Tokenæ•°é‡: {original_tokens}")
    
    # æˆªæ–­åˆ°100ä¸ªtoken
    truncated_text = token_manager.truncate_text(long_text, 100)
    truncated_tokens = count_tokens(truncated_text)
    print(f"æˆªæ–­åTokenæ•°é‡: {truncated_tokens}")
    print(f"æˆªæ–­æ˜¯å¦æˆåŠŸ: {truncated_tokens <= 100}")
    print()

def test_files_truncation():
    """æµ‹è¯•æ–‡ä»¶å†…å®¹æˆªæ–­åŠŸèƒ½"""
    print("=== æµ‹è¯•æ–‡ä»¶å†…å®¹æˆªæ–­åŠŸèƒ½ ===")
    
    # æ¨¡æ‹Ÿæ–‡ä»¶æ•°æ®
    files_data = {
        "short_file.py": "print('hello')",
        "long_file.py": "# è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡ä»¶\n" + "print('line')\n" * 1000,
        "medium_file.py": "def function():\n    pass\n" * 100
    }
    
    print("åŸå§‹æ–‡ä»¶Tokenæ•°é‡:")
    for path, content in files_data.items():
        tokens = count_tokens(content)
        print(f"  {path}: {tokens} tokens")
    
    # æˆªæ–­æ–‡ä»¶å†…å®¹
    truncated_files = token_manager.truncate_files_content(files_data, max_tokens_per_file=500)
    
    print("\næˆªæ–­åæ–‡ä»¶Tokenæ•°é‡:")
    for path, content in truncated_files.items():
        tokens = count_tokens(content)
        print(f"  {path}: {tokens} tokens")
    print()

def test_diff_truncation():
    """æµ‹è¯•diffå†…å®¹æˆªæ–­åŠŸèƒ½"""
    print("=== æµ‹è¯•Diffå†…å®¹æˆªæ–­åŠŸèƒ½ ===")
    
    # æ¨¡æ‹Ÿdiffå†…å®¹
    diff_lines = []
    diff_lines.append("@@ -1,10 +1,15 @@")
    for i in range(50):
        diff_lines.append(f"+æ–°å¢è¡Œ {i}")
        diff_lines.append(f"-åˆ é™¤è¡Œ {i}")
        diff_lines.append(f" ä¸Šä¸‹æ–‡è¡Œ {i}")
    
    print(f"åŸå§‹diffè¡Œæ•°: {len(diff_lines)}")
    
    # æˆªæ–­diffå†…å®¹
    truncated_diff = token_manager.truncate_diff_content(diff_lines, max_lines=20)
    print(f"æˆªæ–­ådiffè¡Œæ•°: {len(truncated_diff)}")
    
    # æ£€æŸ¥æ˜¯å¦ä¿ç•™äº†é‡è¦è¡Œ
    important_lines = [line for line in truncated_diff if line.startswith(('+', '-', '@@'))]
    print(f"é‡è¦è¡Œæ•°é‡: {len(important_lines)}")
    print()

def test_prompt_optimization():
    """æµ‹è¯•promptä¼˜åŒ–åŠŸèƒ½"""
    print("=== æµ‹è¯•Promptä¼˜åŒ–åŠŸèƒ½ ===")
    
    # åˆ›å»ºä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡
    long_context = "è¿™æ˜¯æ–‡ä»¶å†…å®¹ã€‚\n" * 10000
    file_listing = "- 0 # file1.py\n- 1 # file2.py"
    base_prompt = "è¯·åˆ†æä»¥ä¸‹ä»£ç ï¼š"
    
    print(f"åŸå§‹ä¸Šä¸‹æ–‡Tokenæ•°é‡: {count_tokens(long_context)}")
    
    # ä¼˜åŒ–prompt
    optimized_context, optimized_prompt = token_manager.optimize_prompt_for_abstractions(
        long_context, file_listing, base_prompt, max_context_tokens=5000
    )
    
    print(f"ä¼˜åŒ–åä¸Šä¸‹æ–‡Tokenæ•°é‡: {count_tokens(optimized_context)}")
    print()

def test_call_llm_with_long_prompt():
    """æµ‹è¯•å¸¦æœ‰é•¿promptçš„LLMè°ƒç”¨"""
    print("=== æµ‹è¯•é•¿Promptçš„LLMè°ƒç”¨ ===")
    
    # åˆ›å»ºä¸€ä¸ªå¯èƒ½è¶…è¿‡tokené™åˆ¶çš„prompt
    long_prompt = "è¯·åˆ†æä»¥ä¸‹ä»£ç ï¼š\n" + "print('test')\n" * 50000
    
    print(f"åŸå§‹prompt Tokenæ•°é‡: {count_tokens(long_prompt)}")
    
    try:
        # è¿™åº”è¯¥ä¼šè‡ªåŠ¨æˆªæ–­prompt
        response = call_llm(long_prompt, use_cache=False, max_tokens=1000)
        print("âœ… LLMè°ƒç”¨æˆåŠŸï¼Œpromptè¢«è‡ªåŠ¨æˆªæ–­")
        print(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
    except Exception as e:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
    print()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•Tokenç®¡ç†åŠŸèƒ½")
    print("=" * 60)
    
    test_token_counting()
    test_text_truncation()
    test_files_truncation()
    test_diff_truncation()
    test_prompt_optimization()
    
    # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•ä¼šå®é™…è°ƒç”¨LLMï¼Œå¯èƒ½äº§ç”Ÿè´¹ç”¨
    # test_call_llm_with_long_prompt()
    
    print("ğŸ‰ Tokenç®¡ç†åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")